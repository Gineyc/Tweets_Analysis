{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected  <tweepy.api.API object at 0x00000233C4F42E88>\n"
     ]
    }
   ],
   "source": [
    "# Import the must library\n",
    "import tweepy \n",
    "import csv\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "#input the auth key to connect tweepy api, print result if connected sucessfully, \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CONSUMER_KEY = 'hy2RNcQ3AkHlOYwn3jR0mIn21'\n",
    "CONSUMER_SECRET = 'TPr76aYBC0fkbLs5AVDwoUPWf8o0afm5UzimLEogHHqXUaZCmV'\n",
    "OAUTH_TOKEN = '1095973961475731457-brMqEws4krkDvl5weHlfllqHkREjPl'\n",
    "OAUTH_TOKEN_SECRET = 'vDIcQkJsTohzfSmRDqj38NDYvlSTC1geML95vb4sfn43k'\n",
    "try:\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
    "    auth.set_access_token(OAUTH_TOKEN,OAUTH_TOKEN_SECRET)\n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    print(\"connected \",api)\n",
    "except:\n",
    "    print(\"Error: Authentication Failed\") \n",
    "    \n",
    "def tweets_crawling(hashtag,tweet_mode,lang,result_type,pagecount,items):\n",
    "    df = pd.DataFrame()\n",
    "    msgs = []\n",
    "    msg =[]\n",
    "    for tweet in tweepy.Cursor(api.search,q=hashtag,tweet_mode=tweet_mode,lang=lang,result_type=result_type,count=pagecount).items(items):\n",
    "        if 'retweeted_status' in dir(tweet):\n",
    "            text=tweet.retweeted_status.full_text\n",
    "        else:\n",
    "            text=tweet.full_text\n",
    "        msg=[tweet.user.id,tweet.user.name,tweet.user.location,tweet.user.friends_count,tweet.created_at,tweet.retweet_count,tweet.favorite_count,text]\n",
    "        msgs.append(msg)\n",
    "    df=pd.DataFrame(msgs)\n",
    "    df.columns=['user_id','user_name','user_location','user_friends_count','created_time','retweet_count','favourite_count','original_text']\n",
    "    return df\n",
    "\n",
    "def extract_hashtag(text):\n",
    "    hashtag=[]\n",
    "    for i in text:\n",
    "        a=re.findall(r\"#(\\w+)\", i)\n",
    "        hashtag.append([i.capitalize() for i in a])\n",
    "    return hashtag\n",
    "\n",
    "def data_clean(text):\n",
    "    p=re.compile(r'[-,$()#+&*]')\n",
    "    urlclean=re.compile(r'https://[a-zA-Z0-9.?/&=:]*',re.S)\n",
    "    tagclean=re.compile(r'#[a-zA-Z0-9.?/&=:_]*',re.S)\n",
    "    userclean=re.compile(r'@[a-zA-Z0-9.?/&=:_]*',re.S)\n",
    "    cleaned_text=[]\n",
    "\n",
    "    for i in text:\n",
    "        nourl=re.sub(urlclean,\"\",i)\n",
    "        nouser=re.sub(userclean,\"\",nourl)\n",
    "        notag=re.sub(tagclean,\"\",nouser)\n",
    "        clean =notag.replace('\\n',\"\")\n",
    "        cleaned_text.append(clean)\n",
    "    return cleaned_text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = tweets_crawling(hashtag='#vegan',tweet_mode='extended',lang='en',result_type='mixed',pagecount=100,items=100)\n",
    "df.to_csv('vegancrawl.csv',header=True,index=True,encoding='utf_8_sig')\n",
    "df['hashtag'] = extract_hashtag(df['original_text'])\n",
    "df['cleaned_text'] = data_clean(df['original_text'])\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>created_time</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>original_text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56814425</td>\n",
       "      <td>espialtoday</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>2314</td>\n",
       "      <td>2020-01-17 17:16:27</td>\n",
       "      <td>1466</td>\n",
       "      <td>0</td>\n",
       "      <td>The feathers needed for bedding, jackets etc. ...</td>\n",
       "      <td>[Govegan, Vegan, Animalrights]</td>\n",
       "      <td>The feathers needed for bedding, jackets etc. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>599677362</td>\n",
       "      <td>Ruby #EndBSL</td>\n",
       "      <td>Deepest Dorset</td>\n",
       "      <td>2517</td>\n",
       "      <td>2020-01-17 17:16:27</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>Are you one less person harming animals?‚Å£‚Å†\\n‚Å£‚Å†...</td>\n",
       "      <td>[Vegan]</td>\n",
       "      <td>Are you one less person harming animals?‚Å£‚Å†‚Å£‚Å†Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1013711024</td>\n",
       "      <td>Luci üòà</td>\n",
       "      <td>UK</td>\n",
       "      <td>4965</td>\n",
       "      <td>2020-01-17 17:16:24</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...</td>\n",
       "      <td>[Win, Crueltyfree, Vegan, Paraffinfree, Amazon...</td>\n",
       "      <td>üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>939963727813775361</td>\n",
       "      <td>Mellamigo</td>\n",
       "      <td>Germany</td>\n",
       "      <td>51</td>\n",
       "      <td>2020-01-17 17:16:21</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>How adorable! See this lucky rescued pig üê∑ enj...</td>\n",
       "      <td>[Vegan, Fridaythoughts, Fridayfeeling]</td>\n",
       "      <td>How adorable! See this lucky rescued pig üê∑ enj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2285825144</td>\n",
       "      <td>Mummy Hasson</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3793</td>\n",
       "      <td>2020-01-17 17:16:04</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...</td>\n",
       "      <td>[Win, Crueltyfree, Vegan, Paraffinfree, Amazon...</td>\n",
       "      <td>üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>19995</td>\n",
       "      <td>2256664812</td>\n",
       "      <td>MRX</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>212</td>\n",
       "      <td>2020-01-15 00:33:20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Awesome DYI Birthday Cupcakes for @harpuranne ...</td>\n",
       "      <td>[Glutenfree, Vegan, Yummmy]</td>\n",
       "      <td>Awesome DYI Birthday Cupcakes for  from  Pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>19996</td>\n",
       "      <td>819697568087044096</td>\n",
       "      <td>USBlog Retweets</td>\n",
       "      <td>United States</td>\n",
       "      <td>1747</td>\n",
       "      <td>2020-01-15 00:33:15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Classic hummus is simple and easy to make, but...</td>\n",
       "      <td>[Vegan, Veganrecipe, Worldcuisine, Blogger, Th...</td>\n",
       "      <td>Classic hummus is simple and easy to make, but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>19997</td>\n",
       "      <td>225278849</td>\n",
       "      <td>Life‚Äôs too Short to Not</td>\n",
       "      <td>Queensland, Australia</td>\n",
       "      <td>2339</td>\n",
       "      <td>2020-01-15 00:33:14</td>\n",
       "      <td>681</td>\n",
       "      <td>0</td>\n",
       "      <td>üò≠ HOW MUCH ANIMAL SUFFERING IS NECESSARY BEFOR...</td>\n",
       "      <td>[Climatecrisis, Veganuary, Veganuary2020, Vega...</td>\n",
       "      <td>üò≠ HOW MUCH ANIMAL SUFFERING IS NECESSARY BEFOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>19998</td>\n",
       "      <td>2256664812</td>\n",
       "      <td>MRX</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>212</td>\n",
       "      <td>2020-01-15 00:32:59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>My happy place, all you can eat sushi. üôå #vega...</td>\n",
       "      <td>[Vegan, Plantbased]</td>\n",
       "      <td>My happy place, all you can eat sushi. üôå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>19999</td>\n",
       "      <td>2256664812</td>\n",
       "      <td>MRX</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>212</td>\n",
       "      <td>2020-01-15 00:32:39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazing plant based lunch planttheorycafeoffic...</td>\n",
       "      <td>[Vegan, Plantbased, Miamimagic]</td>\n",
       "      <td>Amazing plant based lunch planttheorycafeoffic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0             user_id                user_name  \\\n",
       "0               0            56814425              espialtoday   \n",
       "1               1           599677362             Ruby #EndBSL   \n",
       "2               2          1013711024                   Luci üòà   \n",
       "3               3  939963727813775361                Mellamigo   \n",
       "4               4          2285825144             Mummy Hasson   \n",
       "...           ...                 ...                      ...   \n",
       "19995       19995          2256664812                      MRX   \n",
       "19996       19996  819697568087044096          USBlog Retweets   \n",
       "19997       19997           225278849  Life‚Äôs too Short to Not   \n",
       "19998       19998          2256664812                      MRX   \n",
       "19999       19999          2256664812                      MRX   \n",
       "\n",
       "               user_location  user_friends_count         created_time  \\\n",
       "0             Houston, Texas                2314  2020-01-17 17:16:27   \n",
       "1             Deepest Dorset                2517  2020-01-17 17:16:27   \n",
       "2                         UK                4965  2020-01-17 17:16:24   \n",
       "3                    Germany                  51  2020-01-17 17:16:21   \n",
       "4             United Kingdom                3793  2020-01-17 17:16:04   \n",
       "...                      ...                 ...                  ...   \n",
       "19995             New Jersey                 212  2020-01-15 00:33:20   \n",
       "19996          United States                1747  2020-01-15 00:33:15   \n",
       "19997  Queensland, Australia                2339  2020-01-15 00:33:14   \n",
       "19998             New Jersey                 212  2020-01-15 00:32:59   \n",
       "19999             New Jersey                 212  2020-01-15 00:32:39   \n",
       "\n",
       "       retweet_count  favourite_count  \\\n",
       "0               1466                0   \n",
       "1                 49                0   \n",
       "2                151                0   \n",
       "3                108                0   \n",
       "4                151                0   \n",
       "...              ...              ...   \n",
       "19995              1                0   \n",
       "19996              4                0   \n",
       "19997            681                0   \n",
       "19998              1                0   \n",
       "19999              1                0   \n",
       "\n",
       "                                           original_text  \\\n",
       "0      The feathers needed for bedding, jackets etc. ...   \n",
       "1      Are you one less person harming animals?‚Å£‚Å†\\n‚Å£‚Å†...   \n",
       "2      üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...   \n",
       "3      How adorable! See this lucky rescued pig üê∑ enj...   \n",
       "4      üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...   \n",
       "...                                                  ...   \n",
       "19995  Awesome DYI Birthday Cupcakes for @harpuranne ...   \n",
       "19996  Classic hummus is simple and easy to make, but...   \n",
       "19997  üò≠ HOW MUCH ANIMAL SUFFERING IS NECESSARY BEFOR...   \n",
       "19998  My happy place, all you can eat sushi. üôå #vega...   \n",
       "19999  Amazing plant based lunch planttheorycafeoffic...   \n",
       "\n",
       "                                                 hashtag  \\\n",
       "0                         [Govegan, Vegan, Animalrights]   \n",
       "1                                                [Vegan]   \n",
       "2      [Win, Crueltyfree, Vegan, Paraffinfree, Amazon...   \n",
       "3                 [Vegan, Fridaythoughts, Fridayfeeling]   \n",
       "4      [Win, Crueltyfree, Vegan, Paraffinfree, Amazon...   \n",
       "...                                                  ...   \n",
       "19995                        [Glutenfree, Vegan, Yummmy]   \n",
       "19996  [Vegan, Veganrecipe, Worldcuisine, Blogger, Th...   \n",
       "19997  [Climatecrisis, Veganuary, Veganuary2020, Vega...   \n",
       "19998                                [Vegan, Plantbased]   \n",
       "19999                    [Vegan, Plantbased, Miamimagic]   \n",
       "\n",
       "                                            cleaned_text  \n",
       "0      The feathers needed for bedding, jackets etc. ...  \n",
       "1      Are you one less person harming animals?‚Å£‚Å†‚Å£‚Å†Re...  \n",
       "2      üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...  \n",
       "3      How adorable! See this lucky rescued pig üê∑ enj...  \n",
       "4      üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...  \n",
       "...                                                  ...  \n",
       "19995  Awesome DYI Birthday Cupcakes for  from  Pleas...  \n",
       "19996  Classic hummus is simple and easy to make, but...  \n",
       "19997  üò≠ HOW MUCH ANIMAL SUFFERING IS NECESSARY BEFOR...  \n",
       "19998        My happy place, all you can eat sushi. üôå     \n",
       "19999  Amazing plant based lunch planttheorycafeoffic...  \n",
       "\n",
       "[20000 rows x 11 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('vegancrawl.csv',index_col=None)\n",
    "df['hashtag'] = extract_hashtag(df['original_text'])\n",
    "df['cleaned_text'] = data_clean(df['original_text'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>created_time</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>original_text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>textblob_SA</th>\n",
       "      <th>textblob_poly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56814425</td>\n",
       "      <td>espialtoday</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>2314</td>\n",
       "      <td>2020-01-17 17:16:27</td>\n",
       "      <td>1466</td>\n",
       "      <td>0</td>\n",
       "      <td>The feathers needed for bedding, jackets etc. ...</td>\n",
       "      <td>[Govegan, Vegan, Animalrights]</td>\n",
       "      <td>The feathers needed for bedding, jackets etc. ...</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>599677362</td>\n",
       "      <td>Ruby #EndBSL</td>\n",
       "      <td>Deepest Dorset</td>\n",
       "      <td>2517</td>\n",
       "      <td>2020-01-17 17:16:27</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>Are you one less person harming animals?‚Å£‚Å†\\n‚Å£‚Å†...</td>\n",
       "      <td>[Vegan]</td>\n",
       "      <td>Are you one less person harming animals?‚Å£‚Å†‚Å£‚Å†Re...</td>\n",
       "      <td>-0.208333</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1013711024</td>\n",
       "      <td>Luci üòà</td>\n",
       "      <td>UK</td>\n",
       "      <td>4965</td>\n",
       "      <td>2020-01-17 17:16:24</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...</td>\n",
       "      <td>[Win, Crueltyfree, Vegan, Paraffinfree, Amazon...</td>\n",
       "      <td>üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...</td>\n",
       "      <td>0.123864</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>939963727813775361</td>\n",
       "      <td>Mellamigo</td>\n",
       "      <td>Germany</td>\n",
       "      <td>51</td>\n",
       "      <td>2020-01-17 17:16:21</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>How adorable! See this lucky rescued pig üê∑ enj...</td>\n",
       "      <td>[Vegan, Fridaythoughts, Fridayfeeling]</td>\n",
       "      <td>How adorable! See this lucky rescued pig üê∑ enj...</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2285825144</td>\n",
       "      <td>Mummy Hasson</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3793</td>\n",
       "      <td>2020-01-17 17:16:04</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...</td>\n",
       "      <td>[Win, Crueltyfree, Vegan, Paraffinfree, Amazon...</td>\n",
       "      <td>üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...</td>\n",
       "      <td>0.123864</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             user_id     user_name   user_location  \\\n",
       "0           0            56814425   espialtoday  Houston, Texas   \n",
       "1           1           599677362  Ruby #EndBSL  Deepest Dorset   \n",
       "2           2          1013711024        Luci üòà              UK   \n",
       "3           3  939963727813775361     Mellamigo         Germany   \n",
       "4           4          2285825144  Mummy Hasson  United Kingdom   \n",
       "\n",
       "   user_friends_count         created_time  retweet_count  favourite_count  \\\n",
       "0                2314  2020-01-17 17:16:27           1466                0   \n",
       "1                2517  2020-01-17 17:16:27             49                0   \n",
       "2                4965  2020-01-17 17:16:24            151                0   \n",
       "3                  51  2020-01-17 17:16:21            108                0   \n",
       "4                3793  2020-01-17 17:16:04            151                0   \n",
       "\n",
       "                                       original_text  \\\n",
       "0  The feathers needed for bedding, jackets etc. ...   \n",
       "1  Are you one less person harming animals?‚Å£‚Å†\\n‚Å£‚Å†...   \n",
       "2  üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...   \n",
       "3  How adorable! See this lucky rescued pig üê∑ enj...   \n",
       "4  üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...   \n",
       "\n",
       "                                             hashtag  \\\n",
       "0                     [Govegan, Vegan, Animalrights]   \n",
       "1                                            [Vegan]   \n",
       "2  [Win, Crueltyfree, Vegan, Paraffinfree, Amazon...   \n",
       "3             [Vegan, Fridaythoughts, Fridayfeeling]   \n",
       "4  [Win, Crueltyfree, Vegan, Paraffinfree, Amazon...   \n",
       "\n",
       "                                        cleaned_text  textblob_SA  \\\n",
       "0  The feathers needed for bedding, jackets etc. ...     0.117188   \n",
       "1  Are you one less person harming animals?‚Å£‚Å†‚Å£‚Å†Re...    -0.208333   \n",
       "2  üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...     0.123864   \n",
       "3  How adorable! See this lucky rescued pig üê∑ enj...     0.409722   \n",
       "4  üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...     0.123864   \n",
       "\n",
       "  textblob_poly  \n",
       "0      Positive  \n",
       "1      Negative  \n",
       "2      Positive  \n",
       "3      Positive  \n",
       "4      Positive  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textblob_SA=[]\n",
    "textblob_poly=[]\n",
    "for i in df[\"cleaned_text\"]:\n",
    "    analsysis=TextBlob(i)\n",
    "    textblob_SA.append(analsysis.sentiment.polarity)\n",
    "    if analsysis.sentiment.polarity>=0.1:\n",
    "        textblob_poly.append(\"Positive\")\n",
    "    elif analsysis.sentiment.polarity<=-0.1:\n",
    "        textblob_poly.append(\"Negative\")\n",
    "    else:\n",
    "        textblob_poly.append(\"Neutral\")\n",
    "df[\"textblob_SA\"]=textblob_SA\n",
    "df[\"textblob_poly\"]=textblob_poly\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>created_time</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favourite_count</th>\n",
       "      <th>original_text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>textblob_SA</th>\n",
       "      <th>textblob_poly</th>\n",
       "      <th>nltk_vsa</th>\n",
       "      <th>nltk_poly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56814425</td>\n",
       "      <td>espialtoday</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>2314</td>\n",
       "      <td>2020-01-17 17:16:27</td>\n",
       "      <td>1466</td>\n",
       "      <td>0</td>\n",
       "      <td>The feathers needed for bedding, jackets etc. ...</td>\n",
       "      <td>[Govegan, Vegan, Animalrights]</td>\n",
       "      <td>The feathers needed for bedding, jackets etc. ...</td>\n",
       "      <td>0.117188</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.2579</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>599677362</td>\n",
       "      <td>Ruby #EndBSL</td>\n",
       "      <td>Deepest Dorset</td>\n",
       "      <td>2517</td>\n",
       "      <td>2020-01-17 17:16:27</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>Are you one less person harming animals?‚Å£‚Å†\\n‚Å£‚Å†...</td>\n",
       "      <td>[Vegan]</td>\n",
       "      <td>Are you one less person harming animals?‚Å£‚Å†‚Å£‚Å†Re...</td>\n",
       "      <td>-0.208333</td>\n",
       "      <td>Negative</td>\n",
       "      <td>-0.5594</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1013711024</td>\n",
       "      <td>Luci üòà</td>\n",
       "      <td>UK</td>\n",
       "      <td>4965</td>\n",
       "      <td>2020-01-17 17:16:24</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...</td>\n",
       "      <td>[Win, Crueltyfree, Vegan, Paraffinfree, Amazon...</td>\n",
       "      <td>üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...</td>\n",
       "      <td>0.123864</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.9066</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>939963727813775361</td>\n",
       "      <td>Mellamigo</td>\n",
       "      <td>Germany</td>\n",
       "      <td>51</td>\n",
       "      <td>2020-01-17 17:16:21</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>How adorable! See this lucky rescued pig üê∑ enj...</td>\n",
       "      <td>[Vegan, Fridaythoughts, Fridayfeeling]</td>\n",
       "      <td>How adorable! See this lucky rescued pig üê∑ enj...</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.9870</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2285825144</td>\n",
       "      <td>Mummy Hasson</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3793</td>\n",
       "      <td>2020-01-17 17:16:04</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...</td>\n",
       "      <td>[Win, Crueltyfree, Vegan, Paraffinfree, Amazon...</td>\n",
       "      <td>üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...</td>\n",
       "      <td>0.123864</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.9066</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             user_id     user_name   user_location  \\\n",
       "0           0            56814425   espialtoday  Houston, Texas   \n",
       "1           1           599677362  Ruby #EndBSL  Deepest Dorset   \n",
       "2           2          1013711024        Luci üòà              UK   \n",
       "3           3  939963727813775361     Mellamigo         Germany   \n",
       "4           4          2285825144  Mummy Hasson  United Kingdom   \n",
       "\n",
       "   user_friends_count         created_time  retweet_count  favourite_count  \\\n",
       "0                2314  2020-01-17 17:16:27           1466                0   \n",
       "1                2517  2020-01-17 17:16:27             49                0   \n",
       "2                4965  2020-01-17 17:16:24            151                0   \n",
       "3                  51  2020-01-17 17:16:21            108                0   \n",
       "4                3793  2020-01-17 17:16:04            151                0   \n",
       "\n",
       "                                       original_text  \\\n",
       "0  The feathers needed for bedding, jackets etc. ...   \n",
       "1  Are you one less person harming animals?‚Å£‚Å†\\n‚Å£‚Å†...   \n",
       "2  üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...   \n",
       "3  How adorable! See this lucky rescued pig üê∑ enj...   \n",
       "4  üåüFollow + RTüåüTo #win INTRODUCING our 100% NATU...   \n",
       "\n",
       "                                             hashtag  \\\n",
       "0                     [Govegan, Vegan, Animalrights]   \n",
       "1                                            [Vegan]   \n",
       "2  [Win, Crueltyfree, Vegan, Paraffinfree, Amazon...   \n",
       "3             [Vegan, Fridaythoughts, Fridayfeeling]   \n",
       "4  [Win, Crueltyfree, Vegan, Paraffinfree, Amazon...   \n",
       "\n",
       "                                        cleaned_text  textblob_SA  \\\n",
       "0  The feathers needed for bedding, jackets etc. ...     0.117188   \n",
       "1  Are you one less person harming animals?‚Å£‚Å†‚Å£‚Å†Re...    -0.208333   \n",
       "2  üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...     0.123864   \n",
       "3  How adorable! See this lucky rescued pig üê∑ enj...     0.409722   \n",
       "4  üåüFollow + RTüåüTo  INTRODUCING our 100% NATURAL ...     0.123864   \n",
       "\n",
       "  textblob_poly  nltk_vsa nltk_poly  \n",
       "0      Positive    0.2579  Positive  \n",
       "1      Negative   -0.5594  Negative  \n",
       "2      Positive    0.9066  Positive  \n",
       "3      Positive    0.9870  Positive  \n",
       "4      Positive    0.9066  Positive  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_vsa=[]\n",
    "nltk_poly=[]\n",
    "sid=SentimentIntensityAnalyzer()\n",
    "for i in df[\"cleaned_text\"]:\n",
    "    nltk_vsa.append(sid.polarity_scores(i)['compound'])\n",
    "    if sid.polarity_scores(i)['compound']>=0.1:\n",
    "        nltk_poly.append(\"Positive\")\n",
    "    elif sid.polarity_scores(i)['compound']<=-0.1:\n",
    "        nltk_poly.append(\"Negative\")\n",
    "    else:\n",
    "        nltk_poly.append(\"Neutral\")\n",
    "        \n",
    "df[\"nltk_vsa\"]=nltk_vsa\n",
    "df[\"nltk_poly\"]=nltk_poly\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
